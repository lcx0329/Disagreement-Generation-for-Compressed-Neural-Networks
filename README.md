# Diff-Finder

## Usage

The toolkit directory contains our core code.

Display Diff-Finder results

```
python show_result.py
```

We also provide an IPython notebook file to display generated images [here](./show_result.ipynb).

## Experiments

**Comparing Original and Generated Images**:

Our paper presents a table of experimental results, yet the produced images are relatively small and challenging to discern. Therefore, we exhibit high-definition versions of both the original image and those generated by various disagreement generation algorithms, facilitating easier observation of the differences by readers. 

Apparently, the DFLARE algorithm produces the lowest quality images. The DiffChaser algorithm exhibits blurriness in areas with averaged colors, accompanied by colored noise. Meanwhile, the results of the DiffFinder and CW algorithms are almost indistinguishable to the human eye, even in high-definition images.

![Alt text](compare.png)


## Additional Experiments
Apart from the content of the research paper report, we conducted three additional experiments to validate the performance of multiple models on the ImageNet dataset. Due to time and computational resource constraints, we used pre-trained models provided by the official website. Specifically, we used the following models:

1. **ResNet50** and its corresponding quantized model: [PyTorch Vision - ResNet50](https://pytorch.org/vision/stable/models.html?highlight=pretrain)
2. **Inception** and its corresponding quantized model: [PyTorch Vision - Inception](https://pytorch.org/vision/stable/models.html?highlight=pretrain)
3. **ResNeXt-101** and its corresponding quantized model: [PyTorch Vision - ResNeXt-101](https://pytorch.org/vision/stable/models.html?highlight=pretrain)

Here are the detailed performances of each model in the tests:

| Model Name       | Parameter Settings | Iterations | Number of Images | PSNR Value         | SSIM Value         | Success Rate (SR) (%) |
| ---------------- | ------------------ | ---------- | ---------------- | ------------------ | ------------------ | --------------------- |
| ResNet50Qmodel   | 1                  | 50         | 1000             | 48.836             | 0.997              | 99.2                  |
| InceptionQmodel  | 1                  | 200        | 1000             | 46.629 | 0.994 | 97.4                  |
| ResNext101Qmodel | 1                  | 200        | 1000             | 50.869  | 0.998 | 99.4                  |

**Success Rate (SR) Analysis**:


- Diff-Finder demonstrated exceptional performance across all models, with success rates ranging from 97.4% to 99.4%.
- Particularly in the ResNext101Qmodel, Diff-Finder excelled, achieving an impressive success rate of 99.4%.

**Image Quality (SSIM & PSNR) Analysis**:

- Overall, the three quantized models all produced high-quality images.


**Comparing Original and Generated Images on ImageNet**:

![Alt text](df_resnext101.png)

- Here is a display of the ImageNet image collection. In this display, the top row shows the original images, while the bottom row presents the images we generated. From these images, it's evident that it is almost impossible to distinguish between the original and our generated images with the naked eye, significantly demonstrating the high quality of the images we created.



